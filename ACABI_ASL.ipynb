{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNw3P+os9u6wK9Jy88cRoJT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mihaela-manciu/Asigurarea-calitatii-in-aplicatii-bazate-pe-imagine/blob/main/ACABI_ASL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhhWfKGrjgC2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DaVBfEO-lpct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where your images are stored (dataset folder)\n",
        "base_dir = 'D:/TAID/Semestrul 2/ACABI/dataset'\n",
        "\n",
        "# Image dimensions for VGG16\n",
        "img_size = (224, 224)\n"
      ],
      "metadata": {
        "id": "t6TocZxZjlSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create data generators for training, validation, and testing\n",
        "def create_data_generators(base_dir, img_size=(224, 224), batch_size=32):\n",
        "    # ImageDataGenerator with augmentation for training\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Validation and Test generators (no augmentation, only rescaling)\n",
        "    val_test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    # Flow from directory (train, validation, test splits)\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        os.path.join(base_dir, 'train'),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    val_generator = val_test_datagen.flow_from_directory(\n",
        "        os.path.join(base_dir, 'val'),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = val_test_datagen.flow_from_directory(\n",
        "        os.path.join(base_dir, 'test'),\n",
        "        target_size=img_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator, test_generator\n",
        "\n"
      ],
      "metadata": {
        "id": "11Fy_yAljoWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the VGG16 model with custom top layers\n",
        "def build_vgg16_model(input_shape=(224, 224, 3), num_classes=26):\n",
        "    # Load the pre-trained VGG16 model without the top layers (to add our own)\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the base model to avoid retraining it\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom layers on top of the VGG16 base model\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)  # Add dropout for regularization\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Final model\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "_Olp03bKjr4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile the model\n",
        "def compile_model(model):\n",
        "    model.compile(optimizer=Adam(lr=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "uOVGrSb2juf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "def train_model(model, train_generator, val_generator, epochs=10, batch_size=32):\n",
        "    # Define callbacks for saving the best model and early stopping\n",
        "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=epochs,\n",
        "        validation_data=val_generator,\n",
        "        callbacks=[checkpoint, early_stop]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n"
      ],
      "metadata": {
        "id": "2UV82Ckijwwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "def evaluate_model(model, test_generator):\n",
        "    test_loss, test_acc = model.evaluate(test_generator)\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Eu2M7NZIjzjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    # Plot accuracy\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "KGe23Z1Hj51B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main function to put everything together\n",
        "def main():\n",
        "    # Create data generators\n",
        "    train_generator, val_generator, test_generator = create_data_generators(base_dir)\n",
        "\n",
        "    # Build the VGG16 model\n",
        "    model = build_vgg16_model(input_shape=(224, 224, 3), num_classes=26)  # ASL has 26 letters\n",
        "\n",
        "    # Compile the model\n",
        "    compile_model(model)\n",
        "\n",
        "    # Train the model\n",
        "    history = train_model(model, train_generator, val_generator, epochs=20, batch_size=32)\n",
        "\n",
        "    # Plot the training history (accuracy and loss)\n",
        "    plot_training_history(history)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(model, test_generator)\n",
        "\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "gl-zCiSZj8fs",
        "outputId": "7c72192e-751a-4458-c4c0-c6c58d9a5ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'D:/TAID/Semestrul 2/ACABI/dataset/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-06b9897d1d96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-06b9897d1d96>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Create data generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_data_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Build the VGG16 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-81450985e78e>\u001b[0m in \u001b[0;36mcreate_data_generators\u001b[0;34m(base_dir, img_size, batch_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Flow from directory (train, validation, test splits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     train_generator = train_datagen.flow_from_directory(\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0mkeep_aspect_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     ):\n\u001b[0;32m-> 1138\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m   1139\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/legacy/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, keep_aspect_ratio, dtype)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/TAID/Semestrul 2/ACABI/dataset/train'"
          ]
        }
      ]
    }
  ]
}